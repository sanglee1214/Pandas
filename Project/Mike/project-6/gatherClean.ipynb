{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB API + Random Forests\n",
    "\n",
    "This file includes all the initial code that gathers and cleans data into a usable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports at the top\n",
    "import json\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from imdbpie import Imdb\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "from IPython.display import Image\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb = Imdb()\n",
    "imdb = Imdb(anonymize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_entries(site):\\n    response = requests.get(site)\\n    html = response.text\\n    entries = re.findall(\"<a href.*?/title/(.*?)/\", html) #Wrong regex\\n    return list(set(entries))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So this function gets a list of all 100 movies IDs.\n",
    "#However, IMDB.com doesn't like people getting all of their data very easily, so we'll just use this page to get the IDs\n",
    "#So this function doesn't iterate through pages, as all movies in the Bottom 100 are on a single page.\n",
    "#It takes their unique IDs that are encoded in the HTML, and puts them in a list, called 'entries\n",
    "\n",
    "'''\n",
    "def get_entries(site):\n",
    "    response = requests.get(site)\n",
    "    html = response.text\n",
    "    entries = re.findall(\"<a href.*?/title/(.*?)/\", html) #Wrong regex\n",
    "    return list(set(entries))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbottomEntries = get_entries('http://www.imdb.com/chart/bottom')\\ntopEntries = get_entries('http://www.imdb.com/chart/top')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bottomEntries = get_entries('http://www.imdb.com/chart/bottom')\n",
    "topEntries = get_entries('http://www.imdb.com/chart/top')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_entry(entry):\\n    res = requests.get('http://www.omdbapi.com/?i='+entry)\\n    if res.status_code != 200:\\n        print entry, res.status_code\\n    else:\\n        print '.',\\n    try:\\n        j = json.loads(res.text)\\n    except ValueError:\\n        j = None\\n    return j\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now that we have the 250 IDs, we need a way to search omdapi (which has gathered all data for each IMDB movie in a \n",
    "#nice little JSON tree). \n",
    "\n",
    "#So we need to scrape each movie's JSON tree with Beautiful soup\n",
    "#Just like with indeed.com, it's going to use omdabpi's search engine 250 times, once for each id in the entries list\n",
    "#from above. After it searches a movie id in the lsit above, it will scrape its JSON tree.\n",
    "'''\n",
    "def get_entry(entry):\n",
    "    res = requests.get('http://www.omdbapi.com/?i='+entry)\n",
    "    if res.status_code != 200:\n",
    "        print entry, res.status_code\n",
    "    else:\n",
    "        print '.',\n",
    "    try:\n",
    "        j = json.loads(res.text)\n",
    "    except ValueError:\n",
    "        j = None\n",
    "    return j\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbottomEntriesDictList = [get_entry(e) for e in bottomEntries]\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So you're going to repreat the function above for every item(movie id) in the 'entries' list\n",
    "#It returns a dictionary that can then be turned into a dataframe\n",
    "\n",
    "'''\n",
    "bottomEntriesDictList = [get_entry(e) for e in bottomEntries]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbottom100 = pd.DataFrame(bottomEntriesDictList)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we turn the JSON file for each of th 100 movies into a dataframe\n",
    "'''\n",
    "bottom100 = pd.DataFrame(bottomEntriesDictList)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntopEntriesDictList = [get_entry(e) for e in topEntries]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So you're going to repreat the function above for every item(movie id) in the 'entries' list\n",
    "#It returns a dictionary that can then be turned into a dataframe\n",
    "'''\n",
    "topEntriesDictList = [get_entry(e) for e in topEntries]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntop250 = pd.DataFrame(topEntriesDictList)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we turn the JSON file for each of th 100 movies into a dataframe\n",
    "\n",
    "'''\n",
    "top250 = pd.DataFrame(topEntriesDictList)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_gross(entry): #define the function\\n    response = requests.get(\\'http://www.imdb.com/title/\\'+entry) #This will generate a request from the page for an entry\\n    html = response.text\\n    try:\\n        gross_list = re.findall(\"Gross:</h4>[ ]*\\\\$([^ ]*)\", html) #This will create a list with the value after the word \\'Gross\\'\\n        gross = int(gross_list[0].replace(\\',\\', \\'\\')) #This creates a new value by convertinf the above to an integer and eliminating commas\\n        return gross\\n    except Exception as ex:\\n        return None\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There is still some information we would want, but OMDb API does not provide\n",
    "#So, we have to go back to imdb.com to scrape the gross revenue for each movie\n",
    "#This function will ultimately search for each movie by their id in the entries list, and scrape the gross revenue into\n",
    "#a new list called 'grosses\n",
    "\n",
    "'''\n",
    "def get_gross(entry): #define the function\n",
    "    response = requests.get('http://www.imdb.com/title/'+entry) #This will generate a request from the page for an entry\n",
    "    html = response.text\n",
    "    try:\n",
    "        gross_list = re.findall(\"Gross:</h4>[ ]*\\$([^ ]*)\", html) #This will create a list with the value after the word 'Gross'\n",
    "        gross = int(gross_list[0].replace(',', '')) #This creates a new value by convertinf the above to an integer and eliminating commas\n",
    "        return gross\n",
    "    except Exception as ex:\n",
    "        return None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbottomGrosses = [(e, get_gross(e)) for e in bottomEntries]#Repeat the function above for each id in the entries list\\ntopGrosses = [(e, get_gross(e)) for e in topEntries]\\nbottomGrosses = pd.DataFrame(bottomGrosses, columns=[\\'imdbID\\', \\'gross\\'])\\ntopGrosses = pd.DataFrame(topGrosses, columns=[\\'imdbID\\', \\'gross\\'])\\nbottomGrosses[\"gross\"] = bottomGrosses[\"gross\"].fillna(bottomGrosses[\"gross\"].mean())\\ntopGrosses[\"gross\"] = topGrosses[\"gross\"].fillna(topGrosses[\"gross\"].mean())\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bottomGrosses = [(e, get_gross(e)) for e in bottomEntries]#Repeat the function above for each id in the entries list\n",
    "topGrosses = [(e, get_gross(e)) for e in topEntries]\n",
    "bottomGrosses = pd.DataFrame(bottomGrosses, columns=['imdbID', 'gross'])\n",
    "topGrosses = pd.DataFrame(topGrosses, columns=['imdbID', 'gross'])\n",
    "bottomGrosses[\"gross\"] = bottomGrosses[\"gross\"].fillna(bottomGrosses[\"gross\"].mean())\n",
    "topGrosses[\"gross\"] = topGrosses[\"gross\"].fillna(topGrosses[\"gross\"].mean())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef cleanData(df, df1):\\n    movieType = []\\n    df.drop(['Actors','Awards','Country','Director','Genre','Language','Metascore',\\n             'Plot','Poster','Rated','Released','Response','Type','Writer'], axis =1 ,inplace=True)\\n    df = df[df.Runtime != 'N/A']\\n    for row in ['Runtime']:\\n        df['Runtime'] = df['Runtime'].str.rstrip('min').astype(float)\\n    for row in ['Year']:\\n        df['Year'] = df['Year'].astype(int)\\n    for row in ['imdbRating']:\\n        df['imdbRating'] = df['imdbRating'].astype(float)\\n    for row in ['imdbVotes']:\\n        df['imdbVotes'] = df['imdbVotes'].replace(',','',regex=True).astype(float)\\n    for row in df['imdbRating']:\\n        if row <= 3:\\n            movieType.append(0)\\n        else:\\n            movieType.append(1)\\n    df['movieType'] = movieType\\n    df = df.rename(columns = {'imdbID'      :'imdbID',\\n                                  'Title'       :'title',\\n                                  'Year'        :'year',\\n                                  'Runtime'     :'runtime',\\n                                  'imdbVotes'   :'imdbVotes',\\n                                  'imdbRating'  :'imdbRating',\\n                                  'movieType'   :'movieType'})\\n    df = df[['imdbID', 'movieType', 'title', 'year', 'runtime', 'imdbVotes', 'imdbRating']]\\n    df = pd.merge(df, df1)\\n    return df\\n\\ntop250 = cleanData(top250, topGrosses)\\nbottom100 = cleanData(bottom100, bottomGrosses)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def cleanData(df, df1):\n",
    "    movieType = []\n",
    "    df.drop(['Actors','Awards','Country','Director','Genre','Language','Metascore',\n",
    "             'Plot','Poster','Rated','Released','Response','Type','Writer'], axis =1 ,inplace=True)\n",
    "    df = df[df.Runtime != 'N/A']\n",
    "    for row in ['Runtime']:\n",
    "        df['Runtime'] = df['Runtime'].str.rstrip('min').astype(float)\n",
    "    for row in ['Year']:\n",
    "        df['Year'] = df['Year'].astype(int)\n",
    "    for row in ['imdbRating']:\n",
    "        df['imdbRating'] = df['imdbRating'].astype(float)\n",
    "    for row in ['imdbVotes']:\n",
    "        df['imdbVotes'] = df['imdbVotes'].replace(',','',regex=True).astype(float)\n",
    "    for row in df['imdbRating']:\n",
    "        if row <= 3:\n",
    "            movieType.append(0)\n",
    "        else:\n",
    "            movieType.append(1)\n",
    "    df['movieType'] = movieType\n",
    "    df = df.rename(columns = {'imdbID'      :'imdbID',\n",
    "                                  'Title'       :'title',\n",
    "                                  'Year'        :'year',\n",
    "                                  'Runtime'     :'runtime',\n",
    "                                  'imdbVotes'   :'imdbVotes',\n",
    "                                  'imdbRating'  :'imdbRating',\n",
    "                                  'movieType'   :'movieType'})\n",
    "    df = df[['imdbID', 'movieType', 'title', 'year', 'runtime', 'imdbVotes', 'imdbRating']]\n",
    "    df = pd.merge(df, df1)\n",
    "    return df\n",
    "\n",
    "top250 = cleanData(top250, topGrosses)\n",
    "bottom100 = cleanData(bottom100, bottomGrosses)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntop250.to_csv('../assets/06-project6-assets/data/top250.csv', encoding='utf8', index=False)\\nbottom100.to_csv('../assets/06-project6-assets/data/bottom100.csv', encoding='utf8', index=False)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "top250.to_csv('../assets/06-project6-assets/data/top250.csv', encoding='utf8', index=False)\n",
    "bottom100.to_csv('../assets/06-project6-assets/data/bottom100.csv', encoding='utf8', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'top250MovieIDs = top250.imdbID.values.tolist()\\nbottom100MovieIDs = bottom100.imdbID.values.tolist()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we need to scrape the reviews for each of our movie collections, but put them in a different dataframe\n",
    "#So first, we put the imdbIDs in their respective lists so we can iterate through them when scraping reviews\n",
    "#We need the ID again so we can use it as the common key with which we can join tablesl ater\n",
    "\n",
    "'''top250MovieIDs = top250.imdbID.values.tolist()\n",
    "bottom100MovieIDs = bottom100.imdbID.values.tolist()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntop250Reviews = []\\ntop250IDs = []\\nfor x in top250MovieIDs: #For every ID in the ID list\\n    review = imdb.get_title_reviews(x, max_results=15) #We take a list of 15 reviews\\n    for i in review: #For every review in the list of reviews\\n        top250IDs.append(x) #We add that reviews id to one list \\n        top250Reviews.append(i.text) #and the review to another, so they all correspond\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "top250Reviews = []\n",
    "top250IDs = []\n",
    "for x in top250MovieIDs: #For every ID in the ID list\n",
    "    review = imdb.get_title_reviews(x, max_results=15) #We take a list of 15 reviews\n",
    "    for i in review: #For every review in the list of reviews\n",
    "        top250IDs.append(x) #We add that reviews id to one list \n",
    "        top250Reviews.append(i.text) #and the review to another, so they all correspond\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntop250ReviewData = pd.DataFrame({\"imdbID\": top250IDs, \"reviews\": top250Reviews})\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Turn those two lists into a dataframe with the ID and 15 reviews for each ID\n",
    "\n",
    "'''\n",
    "top250ReviewData = pd.DataFrame({\"imdbID\": top250IDs, \"reviews\": top250Reviews})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbottom100Reviews = []\\nbottom100IDs = []\\nfor x in bottom100MovieIDs:\\n    review = imdb.get_title_reviews(x, max_results=15)\\n    for i in review:\\n        bottom100IDs.append(x)\\n        bottom100Reviews.append(i.text)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We repeat the process above, except with the bottom 100\n",
    "#We don't want to combine these dataframes yet, because we want the top 50 adjectives used to describe\n",
    "#the worst and best movies, and see to which extent there is overlap or exclusivity in the ways\n",
    "#people describe good and bad movies\n",
    "\n",
    "'''\n",
    "bottom100Reviews = []\n",
    "bottom100IDs = []\n",
    "for x in bottom100MovieIDs:\n",
    "    review = imdb.get_title_reviews(x, max_results=15)\n",
    "    for i in review:\n",
    "        bottom100IDs.append(x)\n",
    "        bottom100Reviews.append(i.text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbottom100ReviewData = pd.DataFrame({\"imdbID\": bottom100IDs, \"reviews\": bottom100Reviews})\\ntokenizer = RegexpTokenizer(\\'\\\\w+|\\\\$[\\\\d\\\\.]+|\\\\S+\\')\\ntop250Tokens = [tokenizer.tokenize(review) for review in top250Reviews]\\ntop250PosTokens = [nltk.tag.pos_tag(token) for token in top250Tokens]\\nbottom100Tokens = [nltk.word_tokenize(review) for review in bottom100Reviews]\\nbottom100PosTokens = [nltk.tag.pos_tag(token) for token in bottom100Tokens]\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we need to isolate each word and tag it with its respective part of speech\n",
    "'''\n",
    "bottom100ReviewData = pd.DataFrame({\"imdbID\": bottom100IDs, \"reviews\": bottom100Reviews})\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "top250Tokens = [tokenizer.tokenize(review) for review in top250Reviews]\n",
    "top250PosTokens = [nltk.tag.pos_tag(token) for token in top250Tokens]\n",
    "bottom100Tokens = [nltk.word_tokenize(review) for review in bottom100Reviews]\n",
    "bottom100PosTokens = [nltk.tag.pos_tag(token) for token in bottom100Tokens]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntop250AdjList = []\\nfor x in top250PosTokens:\\n    # each x is a list of (word, POS tag) tuples\\n    for word, pos in x:\\n        if pos in ['JJ', 'JJS', 'JJR']: # feel free to add any other tags you may be looking for\\n            top250AdjList.append(word)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we need to go through the part of speech list, and isolate the adjectives, since they're the descriptive words\n",
    "\n",
    "'''\n",
    "top250AdjList = []\n",
    "for x in top250PosTokens:\n",
    "    # each x is a list of (word, POS tag) tuples\n",
    "    for word, pos in x:\n",
    "        if pos in ['JJ', 'JJS', 'JJR']: # feel free to add any other tags you may be looking for\n",
    "            top250AdjList.append(word)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntop250CommonAdj= [a for a, b in Counter(top250AdjList).most_common(50)]\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "top250CommonAdj= [a for a, b in Counter(top250AdjList).most_common(50)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbottom100AdjList = []\\nfor x in bottom100PosTokens:\\n    # each x is either a list of (word, POS tag) tuples\\n    for word, pos in x:\\n        if pos in ['JJ', 'JJS', 'JJR']: # feel free to add any other tags you may be looking for\\n            bottom100AdjList.append(word)\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeat process for the bottom 100 movies\n",
    "'''\n",
    "bottom100AdjList = []\n",
    "for x in bottom100PosTokens:\n",
    "    # each x is either a list of (word, POS tag) tuples\n",
    "    for word, pos in x:\n",
    "        if pos in ['JJ', 'JJS', 'JJR']: # feel free to add any other tags you may be looking for\n",
    "            bottom100AdjList.append(word)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbottom100CommonAdj= [a for a, b in Counter(bottom100AdjList).most_common(50)]\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bottom100CommonAdj= [a for a, b in Counter(bottom100AdjList).most_common(50)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmovieAdj = bottom100CommonAdj + top250CommonAdj\\nmovieAdj.remove(\"\\'t\")\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine these 2 lists and remove the awkward variable\n",
    "'''\n",
    "movieAdj = bottom100CommonAdj + top250CommonAdj\n",
    "movieAdj.remove(\"'t\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmovieDescriptors = pd.DataFrame(columns=movieAdj)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dataframe that places each descriptor as an index\n",
    "#We see that a top 50 adjective is the letter 't.' Not sure what the pos tagger is doing there, so we'll drop the col\n",
    "'''\n",
    "movieDescriptors = pd.DataFrame(columns=movieAdj)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntop250Movies = pd.DataFrame(top250ReviewData)\\ntop250Movies = top250Movies.join(movieDescriptors)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#append THAT dataframe to each reviews dataframe\n",
    "#Now we want a dataframe that joins the Review data (movieID, 15reviews each), from above with these descriptors\n",
    "'''\n",
    "top250Movies = pd.DataFrame(top250ReviewData)\n",
    "top250Movies = top250Movies.join(movieDescriptors)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbottom100Movies = pd.DataFrame(bottom100ReviewData)\\nbottom100Movies = bottom100Movies.join(movieDescriptors)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bottom100Movies = pd.DataFrame(bottom100ReviewData)\n",
    "bottom100Movies = bottom100Movies.join(movieDescriptors)\n",
    "'''\n",
    "#We see right now it's just filled with NaN values, so we'll populate the cells in the loop below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef getDescriptors(df):\\n    for c, col in enumerate(df.columns[2:]):\\n        for r, row in enumerate(df.index):\\n            reviewLower = df.loc[row,\"reviews\"].lower()\\n            if (col in reviewLower):\\n                df.loc[row,col] = 1\\n            else:\\n                df.loc[row,col] = 0\\n\\ngetDescriptors(top250Movies)\\ngetDescriptors(bottom100Movies)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create function to loop through each word in each review\n",
    "'''\n",
    "def getDescriptors(df):\n",
    "    for c, col in enumerate(df.columns[2:]):\n",
    "        for r, row in enumerate(df.index):\n",
    "            reviewLower = df.loc[row,\"reviews\"].lower()\n",
    "            if (col in reviewLower):\n",
    "                df.loc[row,col] = 1\n",
    "            else:\n",
    "                df.loc[row,col] = 0\n",
    "\n",
    "getDescriptors(top250Movies)\n",
    "getDescriptors(bottom100Movies)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntop250Movies.drop(['reviews'], axis =1 ,inplace=True)\\nbottom100Movies.drop(['reviews'], axis =1 ,inplace=True)\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to join both of these dataframes to the original dataframes that have more info about each movie ID\n",
    "#First, though, we need to drop the 'Reviews' column because we don't need all of that text\n",
    "\n",
    "'''\n",
    "top250Movies.drop(['reviews'], axis =1 ,inplace=True)\n",
    "bottom100Movies.drop(['reviews'], axis =1 ,inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntop250MoviesCopy = top250Movies.groupby([\"imdbID\"], group_keys=False, as_index=False).apply(lambda x: x.iloc[:,1:].max())\\nbottom100MoviesCopy = bottom100Movies.groupby([\"imdbID\"], group_keys=False, as_index=False).apply(lambda x: x.iloc[:,1:].max())\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Then we need to groupby imdbID so we can actually join the tables.\n",
    "#Currently, the review data has 15 rows for each id, while the original movie data only has 1\n",
    "\n",
    "'''\n",
    "top250MoviesCopy = top250Movies.groupby([\"imdbID\"], group_keys=False, as_index=False).apply(lambda x: x.iloc[:,1:].max())\n",
    "bottom100MoviesCopy = bottom100Movies.groupby([\"imdbID\"], group_keys=False, as_index=False).apply(lambda x: x.iloc[:,1:].max())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntop250Movies.to_csv('../assets/06-project6-assets/data/top250Descriptors.csv', encoding='utf8', index=False)\\nbottom100Movies.to_csv('../assets/06-project6-assets/data/bottom100Descriptors.csv', encoding='utf8', index=False)\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before we go any further, let's save these sentiment tables to their own .csv files\n",
    "\n",
    "'''\n",
    "top250Movies.to_csv('../assets/06-project6-assets/data/top250Descriptors.csv', encoding='utf8', index=False)\n",
    "bottom100Movies.to_csv('../assets/06-project6-assets/data/bottom100Descriptors.csv', encoding='utf8', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngoodMovies = top250.join(top250MoviesCopy)\\nbadMovies = bottom100.join(bottom100MoviesCopy)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine them to the tables with the rest of thir info\n",
    "\n",
    "'''\n",
    "goodMovies = top250.join(top250MoviesCopy)\n",
    "badMovies = bottom100.join(bottom100MoviesCopy)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ngoodMovies.to_csv('../assets/06-project6-assets/data/top250MoviesDescriptors.csv', encoding='utf8', index=False)\\nbadMovies.to_csv('../assets/06-project6-assets/data/bottom100MoviesDescriptors.csv', encoding='utf8', index=False)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can save these to .csv files to avoid all of that scraping above\n",
    "\n",
    "'''\n",
    "goodMovies.to_csv('../assets/06-project6-assets/data/top250MoviesDescriptors.csv', encoding='utf8', index=False)\n",
    "badMovies.to_csv('../assets/06-project6-assets/data/bottom100MoviesDescriptors.csv', encoding='utf8', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmovies = pd.concat([goodMovies, badMovies])\\nmovies.reset_index(inplace=True, drop=True)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine them into a master using the copy - should have 348, 107 shape\n",
    "'''\n",
    "movies = pd.concat([goodMovies, badMovies])\n",
    "movies.reset_index(inplace=True, drop=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save that master\n",
    "\n",
    "#movies.to_csv('../assets/06-project6-assets/data/movies.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
